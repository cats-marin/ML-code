{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ba1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Iterable, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class AdamAbs(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[torch.nn.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[callable] = None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        loss = closure() if closure is not None else None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            eps = group[\"eps\"]\n",
    "            lr = group[\"lr\"]\n",
    "            wd = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"AdamAbs does not support sparse gradients\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if not state:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state[\"exp_avg_abs\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_abs = state[\"exp_avg_abs\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "\n",
    "                if wd != 0.0:\n",
    "                    grad = grad.add(p, alpha=wd)\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_abs.mul_(beta2).add_(grad.abs(), alpha=1 - beta2)\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** step\n",
    "                bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "                denom = exp_avg_abs / bias_correction2\n",
    "                step_size = lr / bias_correction1\n",
    "\n",
    "                p.addcdiv_(exp_avg, denom.add(eps), value=-step_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff63a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
