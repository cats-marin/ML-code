{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueQ-9xWeF4Le",
        "outputId": "4c40ec5e-9ea9-4d08-cfae-83c5ac1c935f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1) (75.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install triton==3.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym2QwMfzl0lf",
        "outputId": "33226ea6-ded8-4c63-db9f-3c818043eff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.7.1 in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.1) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch==2.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8DL4w3AM8CPo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL5Wb5aGlVIf",
        "outputId": "6d5413be-2d37-4e42-c3a7-c0e85674b532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1+cu126\n",
            "3.3.1\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(triton.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vNzcGCTr8I5o"
      },
      "outputs": [],
      "source": [
        "@triton.autotune(\n",
        "    configs=[\n",
        "        triton.Config({'BLOCK_D': 16}, num_warps=1),\n",
        "        triton.Config({'BLOCK_D': 16}, num_warps=2),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 32}, num_warps=2),\n",
        "        triton.Config({'BLOCK_D': 32}, num_warps=4),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 64}, num_warps=1),\n",
        "        triton.Config({'BLOCK_D': 64}, num_warps=2),\n",
        "        triton.Config({'BLOCK_D': 64}, num_warps=4),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 128}, num_warps=2),\n",
        "        triton.Config({'BLOCK_D': 128}, num_warps=4),\n",
        "        triton.Config({'BLOCK_D': 128}, num_warps=8),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 256}, num_warps=4),\n",
        "        triton.Config({'BLOCK_D': 256}, num_warps=8),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 512}, num_warps=4),\n",
        "        triton.Config({'BLOCK_D': 512}, num_warps=8),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 1024}, num_warps=8),\n",
        "        triton.Config({'BLOCK_D': 1024}, num_warps=16),\n",
        "    ],\n",
        "    key=['HEAD_DIM', 'ROPE_DIM', 'NUM_ELEMENTS'],\n",
        ")\n",
        "\n",
        "@triton.jit\n",
        "def _rope_fused_kernel(\n",
        "    x_ptr, cos_ptr, sin_ptr, out_ptr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    ROPE_DIM: tl.constexpr,\n",
        "    NUM_ELEMENTS: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    offs_d = tl.arange(0, BLOCK_D)\n",
        "    offs = tl.program_id(axis=0) * BLOCK_D + offs_d\n",
        "\n",
        "    is_in_bounds = offs < NUM_ELEMENTS\n",
        "    x = tl.load(x_ptr + offs, mask=is_in_bounds, other=0.0)\n",
        "\n",
        "    ROPE_OFFSET: tl.constexpr = HEAD_DIM - ROPE_DIM\n",
        "    offs_rope = (offs_d % HEAD_DIM) - ROPE_OFFSET\n",
        "\n",
        "    HALF_ROPE_DIM: tl.constexpr = ROPE_DIM // 2\n",
        "    is_first_half = offs_rope < HALF_ROPE_DIM\n",
        "    rope_partner = tl.gather(x, offs_d + tl.where(is_first_half, HALF_ROPE_DIM, -HALF_ROPE_DIM), axis=0) # can also just use tl.load but tl.gather is more suited for this\n",
        "\n",
        "    use_rope = offs_rope >= 0\n",
        "    cos = tl.load(cos_ptr + offs_rope, mask=use_rope, other=0.0)\n",
        "    sin = tl.load(sin_ptr + offs_rope, mask=use_rope, other=0.0)\n",
        "\n",
        "    out = tl.where(use_rope, x * cos + tl.where(is_first_half, -1.0,  1.0) * rope_partner * sin, x)\n",
        "    tl.store(out_ptr + offs, out, mask=is_in_bounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzjWr-0jCpzH"
      },
      "outputs": [],
      "source": [
        "# Kernel generated by torch.compile\n",
        "\n",
        "# import triton\n",
        "# import triton.language as tl\n",
        "# from triton.compiler.compiler import AttrsDescriptor\n",
        "\n",
        "# from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
        "# from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
        "# from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
        "# triton_helpers.set_driver_to_gpu()\n",
        "\n",
        "# @triton_heuristics.pointwise(\n",
        "#     size_hints={'x': 16777216},\n",
        "#     filename=__file__,\n",
        "#     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=40, cc=75, major=7, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1024, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor.from_dict({'arg_properties': {'tt.divisibility': (0, 1, 2, 3, 4), 'tt.equal_to': ()}, 'cls': 'AttrsDescriptor'})]},\n",
        "#     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': '9182018CCD6A4F758231D68D0B1E1E23CEBB32E5D78CB36B65791C4EB96774A2', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\n",
        "#     min_elem_per_thread=0\n",
        "# )\n",
        "# @triton.jit\n",
        "# def triton_poi_fused_cat_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
        "#     xnumel = 16777216\n",
        "#     xoffset = tl.program_id(0) * XBLOCK\n",
        "#     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
        "#     xmask = tl.full([XBLOCK], True, tl.int1)\n",
        "#     x0 = (xindex % 64)\n",
        "#     x1 = xindex // 64\n",
        "#     x2 = xindex\n",
        "#     tmp0 = x0\n",
        "#     tmp1 = tl.full([1], 0, tl.int64)\n",
        "#     tmp2 = tmp0 >= tmp1\n",
        "#     tmp3 = tl.full([1], 16, tl.int64)\n",
        "#     tmp4 = tmp0 < tmp3\n",
        "#     tmp5 = tl.load(in_ptr0 + (64*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp6 = tmp0 >= tmp3\n",
        "#     tmp7 = tl.full([1], 64, tl.int64)\n",
        "#     tmp8 = tmp0 < tmp7\n",
        "#     tmp9 = tl.load(in_ptr0 + (16 + 64*x1 + ((-16) + x0)), tmp6, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp10 = tl.load(in_ptr1 + ((-16) + x0), tmp6, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp11 = tmp9 * tmp10\n",
        "#     tmp12 = (-16) + x0\n",
        "#     tmp13 = tl.full([1], 0, tl.int64)\n",
        "#     tmp14 = tmp12 >= tmp13\n",
        "#     tmp15 = tl.full([1], 24, tl.int64)\n",
        "#     tmp16 = tmp12 < tmp15\n",
        "#     tmp17 = tmp16 & tmp6\n",
        "#     tmp18 = tl.load(in_ptr0 + (40 + 64*x1 + ((-16) + x0)), tmp17, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp19 = -tmp18\n",
        "#     tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)\n",
        "#     tmp21 = tl.where(tmp17, tmp19, tmp20)\n",
        "#     tmp22 = tmp12 >= tmp15\n",
        "#     tmp23 = tl.full([1], 48, tl.int64)\n",
        "#     tmp24 = tmp12 < tmp23\n",
        "#     tmp25 = tmp22 & tmp6\n",
        "#     tmp26 = tl.load(in_ptr0 + (16 + 64*x1 + ((-24) + ((-16) + x0))), tmp25, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp27 = tl.where(tmp16, tmp21, tmp26)\n",
        "#     tmp28 = tl.load(in_ptr2 + ((-16) + x0), tmp6, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp29 = tmp27 * tmp28\n",
        "#     tmp30 = tmp11 + tmp29\n",
        "#     tmp31 = tl.full(tmp30.shape, 0.0, tmp30.dtype)\n",
        "#     tmp32 = tl.where(tmp6, tmp30, tmp31)\n",
        "#     tmp33 = tl.where(tmp4, tmp5, tmp32)\n",
        "#     tl.store(out_ptr0 + (x2), tmp33, None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LfiC9vbO8Uqs"
      },
      "outputs": [],
      "source": [
        "def apply_rotary_pos_emb_triton(\n",
        "    x: torch.Tensor,\n",
        "    cos_sin: Tuple[torch.Tensor, torch.Tensor],\n",
        ") -> torch.Tensor:\n",
        "    if x.device.type != \"cuda\":\n",
        "        raise RuntimeError(\"Triton kernel requires CUDA tensor\")\n",
        "    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n",
        "        raise TypeError(\"x must be fp16, bf16, or fp32\")\n",
        "\n",
        "    cos, sin = cos_sin\n",
        "\n",
        "    head_dim = x.size(-1)\n",
        "    rope_dim = cos.size(-1)\n",
        "\n",
        "    if rope_dim % 2:\n",
        "        raise ValueError(\"rope_dim must be even\")\n",
        "    if rope_dim > head_dim:\n",
        "        raise ValueError(\"rope_dim should be less than or equal to head_dim\")\n",
        "\n",
        "    out = torch.empty_like(x)\n",
        "    x_numel = x.numel()\n",
        "\n",
        "    grid = lambda META: (triton.cdiv(x_numel, META['BLOCK_D']),)\n",
        "\n",
        "    _rope_fused_kernel[grid](\n",
        "        x, cos, sin, out,\n",
        "        head_dim,\n",
        "        rope_dim,\n",
        "        x_numel,\n",
        "    )\n",
        "\n",
        "    return out.view_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NtKYRBeY8WCQ"
      },
      "outputs": [],
      "source": [
        "def apply_rotary_pos_emb(\n",
        "    x: torch.Tensor, cos_sin: tuple[torch.Tensor, torch.Tensor]\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    cos, sin = cos_sin\n",
        "\n",
        "    head_dim = x.size(-1)\n",
        "    rope_dim = cos.size(-1)\n",
        "\n",
        "    if head_dim == rope_dim:\n",
        "        x = (x * cos) + (_rotate_half(x) * sin)\n",
        "    elif rope_dim < head_dim:\n",
        "        x_nope, x_rope = x.split((head_dim - rope_dim, rope_dim), dim=-1)\n",
        "        x_rope = (x_rope * cos) + (_rotate_half(x_rope) * sin)\n",
        "        x = torch.cat([x_nope, x_rope], dim=-1)\n",
        "    else:\n",
        "        raise ValueError(\"rope_dim should be less than head_dim\")\n",
        "\n",
        "    return x\n",
        "\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QLrf8Jo8XdN",
        "outputId": "0dd18fe3-3786-4271-d182-c5b66c933b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max|diff| = 9.5367431640625e-07\n",
            "max|diff| = 0.0\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    batch_size = 32\n",
        "    seq_len = 1024\n",
        "    num_heads = 8\n",
        "    head_dim = 64\n",
        "    rope_dim = 48\n",
        "\n",
        "    x = torch.randn(batch_size, seq_len, num_heads, head_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    cos = torch.randn(rope_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    sin = torch.randn_like(cos)\n",
        "\n",
        "    ref = apply_rotary_pos_emb(x, (cos, sin))\n",
        "    tri = apply_rotary_pos_emb_triton(x, (cos, sin))\n",
        "\n",
        "    diff = (ref - tri).abs().max()\n",
        "    print(\"max|diff| =\", diff.item())\n",
        "\n",
        "    ref = torch.compile(apply_rotary_pos_emb)(x, (cos, sin))\n",
        "    tri = torch.compile(apply_rotary_pos_emb_triton)(x, (cos, sin))\n",
        "\n",
        "    diff = (ref - tri).abs().max()\n",
        "    print(\"max|diff| =\", diff.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3np4GAH-Vc8",
        "outputId": "29bbc34b-43f0-4655-c5e4-9601579ba96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7671679854393005\n",
            "0.98716801404953\n",
            "\n",
            "0.6861119866371155\n",
            "0.7167999744415283\n",
            "\n",
            "0.6747519969940186\n",
            "0.6750079989433289\n",
            "\n",
            "0.6701440215110779\n",
            "0.6723840236663818\n",
            "\n",
            "0.6635519862174988\n",
            "0.6817600131034851\n",
            "\n",
            "0.6696959733963013\n",
            "0.6689280271530151\n",
            "\n",
            "0.6635839939117432\n",
            "0.6801279783248901\n",
            "\n",
            "0.6614720225334167\n",
            "0.6581760048866272\n",
            "\n",
            "0.6609280109405518\n",
            "0.6921600103378296\n",
            "\n",
            "0.6594560146331787\n",
            "0.6758400201797485\n",
            "\n",
            "0.6635519862174988\n",
            "0.6625919938087463\n",
            "\n",
            "0.6914880275726318\n",
            "0.8195199966430664\n",
            "\n",
            "0.7024959921836853\n",
            "0.6758400201797485\n",
            "\n",
            "0.6696959733963013\n",
            "0.6649600267410278\n",
            "\n",
            "0.6718720197677612\n",
            "0.66975998878479\n",
            "\n",
            "0.6532160043716431\n",
            "0.6584640145301819\n",
            "\n",
            "0.667136013507843\n",
            "0.6657919883728027\n",
            "\n",
            "0.6755520105361938\n",
            "0.7344319820404053\n",
            "\n",
            "0.7309759855270386\n",
            "0.6792320013046265\n",
            "\n",
            "0.6799359917640686\n",
            "0.6750079989433289\n",
            "\n",
            "0.6689599752426147\n",
            "0.6650239825248718\n",
            "\n",
            "0.6717439889907837\n",
            "0.6778879761695862\n",
            "\n",
            "0.6594560146331787\n",
            "0.6625919938087463\n",
            "\n",
            "0.6956160068511963\n",
            "0.7403839826583862\n",
            "\n",
            "0.6778879761695862\n",
            "0.713919997215271\n",
            "\n",
            "0.6533120274543762\n",
            "0.6812480092048645\n",
            "\n",
            "0.6737920045852661\n",
            "0.6771199703216553\n",
            "\n",
            "0.6544640064239502\n",
            "0.6596480011940002\n",
            "\n",
            "0.6589120030403137\n",
            "0.6700800061225891\n",
            "\n",
            "0.6757760047912598\n",
            "0.6801919937133789\n",
            "\n",
            "0.6717439889907837\n",
            "0.6696959733963013\n",
            "\n",
            "0.6570879817008972\n",
            "0.6675519943237305\n",
            "\n",
            "0.6614400148391724\n",
            "0.6615039706230164\n",
            "\n",
            "0.6585919857025146\n",
            "0.6595199704170227\n",
            "\n",
            "0.6664639711380005\n",
            "0.6618880033493042\n",
            "\n",
            "0.668287992477417\n",
            "0.6625279784202576\n",
            "\n",
            "0.6536319851875305\n",
            "0.6550719738006592\n",
            "\n",
            "0.6698560118675232\n",
            "0.6616640090942383\n",
            "\n",
            "0.6632000207901001\n",
            "0.6619840264320374\n",
            "\n",
            "0.6649919748306274\n",
            "0.6611840128898621\n",
            "\n",
            "0.6696959733963013\n",
            "0.681984007358551\n",
            "\n",
            "0.6553599834442139\n",
            "0.6615039706230164\n",
            "\n",
            "0.65830397605896\n",
            "0.6653439998626709\n",
            "\n",
            "0.6583679914474487\n",
            "0.6755200028419495\n",
            "\n",
            "0.6600639820098877\n",
            "0.6696959733963013\n",
            "\n",
            "0.6548799872398376\n",
            "0.6739199757575989\n",
            "\n",
            "0.6656000018119812\n",
            "0.6844800114631653\n",
            "\n",
            "0.6925439834594727\n",
            "0.6969599723815918\n",
            "\n",
            "0.6798719763755798\n",
            "0.681984007358551\n",
            "\n",
            "0.6553599834442139\n",
            "0.6727039813995361\n",
            "\n",
            "0.6574079990386963\n",
            "0.667743980884552\n",
            "\n",
            "0.6557440161705017\n",
            "0.656063973903656\n",
            "\n",
            "0.6529920101165771\n",
            "0.6720319986343384\n",
            "\n",
            "0.6574079990386963\n",
            "0.6584960222244263\n",
            "\n",
            "0.6553599834442139\n",
            "0.6652799844741821\n",
            "\n",
            "0.7065600156784058\n",
            "0.7542719841003418\n",
            "\n",
            "0.6923519968986511\n",
            "0.727616012096405\n",
            "\n",
            "0.7212160229682922\n",
            "0.7311679720878601\n",
            "\n",
            "0.7188479900360107\n",
            "0.764735996723175\n",
            "\n",
            "0.7024639844894409\n",
            "0.7205759882926941\n",
            "\n",
            "0.7003520131111145\n",
            "0.7311040163040161\n",
            "\n",
            "0.7402560114860535\n",
            "0.9625599980354309\n",
            "\n",
            "0.7191359996795654\n",
            "0.73471999168396\n",
            "\n",
            "0.6840320229530334\n",
            "0.6850240230560303\n",
            "\n",
            "0.7148159742355347\n",
            "0.7477440237998962\n",
            "\n",
            "0.7106559872627258\n",
            "0.7186880111694336\n",
            "\n",
            "0.7024639844894409\n",
            "0.725600004196167\n",
            "\n",
            "0.7229440212249756\n",
            "0.7492160201072693\n",
            "\n",
            "0.7209920287132263\n",
            "0.7290880084037781\n",
            "\n",
            "0.694271981716156\n",
            "0.7358080148696899\n",
            "\n",
            "0.6947519779205322\n",
            "0.7284799814224243\n",
            "\n",
            "0.7991999983787537\n",
            "0.7294399738311768\n",
            "\n",
            "0.7256320118904114\n",
            "0.7582719922065735\n",
            "\n",
            "0.7311999797821045\n",
            "0.7272319793701172\n",
            "\n",
            "0.6985920071601868\n",
            "0.7351999878883362\n",
            "\n",
            "0.7045120000839233\n",
            "0.7496320009231567\n",
            "\n",
            "0.7306879758834839\n",
            "0.7297599911689758\n",
            "\n",
            "0.7125440239906311\n",
            "0.7328320145606995\n",
            "\n",
            "0.6984000205993652\n",
            "0.7397119998931885\n",
            "\n",
            "0.7024639844894409\n",
            "0.7265920042991638\n",
            "\n",
            "0.702239990234375\n",
            "0.7298880219459534\n",
            "\n",
            "0.7085760235786438\n",
            "0.7639039754867554\n",
            "\n",
            "0.7108799815177917\n",
            "0.7536640167236328\n",
            "\n",
            "0.6950079798698425\n",
            "0.7134720087051392\n",
            "\n",
            "0.7045120000839233\n",
            "0.7349119782447815\n",
            "\n",
            "0.7129920125007629\n",
            "0.7454400062561035\n",
            "\n",
            "0.7352319955825806\n",
            "0.7331519722938538\n",
            "\n",
            "0.7023680210113525\n",
            "0.7376000285148621\n",
            "\n",
            "0.7033600211143494\n",
            "0.7434560060501099\n",
            "\n",
            "0.7167999744415283\n",
            "0.7537599802017212\n",
            "\n",
            "0.6861439943313599\n",
            "0.7105919718742371\n",
            "\n",
            "0.7130879759788513\n",
            "0.7127040028572083\n",
            "\n",
            "0.6942399740219116\n",
            "0.700543999671936\n",
            "\n",
            "0.6860479712486267\n",
            "0.7109760046005249\n",
            "\n",
            "0.6912000179290771\n",
            "0.7181119918823242\n",
            "\n",
            "0.6907200217247009\n",
            "0.6988480091094971\n",
            "\n",
            "0.6996480226516724\n",
            "0.7047359943389893\n",
            "\n",
            "0.6999679803848267\n",
            "0.7044479846954346\n",
            "\n",
            "0.7008960247039795\n",
            "0.7393280267715454\n",
            "\n",
            "0.6952319741249084\n",
            "0.7577279806137085\n",
            "\n",
            "------------------------------\n",
            "Average ref time over 100 runs: 0.6874 ms\n",
            "Average triton time over 100 runs: 0.7069 ms\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "apply_rotary_pos_emb = torch.compile(apply_rotary_pos_emb)\n",
        "apply_rotary_pos_emb_triton = torch.compile(apply_rotary_pos_emb_triton)\n",
        "\n",
        "batch_size = 32\n",
        "seq_len = 1024\n",
        "num_heads = 8\n",
        "head_dim = 64\n",
        "rope_dim = 64\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, num_heads, head_dim, device=\"cuda\", dtype=torch.float32)\n",
        "cos = torch.randn(rope_dim, device=\"cuda\", dtype=torch.float32)\n",
        "sin = torch.randn_like(cos)\n",
        "\n",
        "# compile for the first time\n",
        "for _ in range(1):\n",
        "    _ = apply_rotary_pos_emb(x, (cos, sin))\n",
        "    _ = apply_rotary_pos_emb_triton(x, (cos, sin))\n",
        "\n",
        "total_ref_time = 0.0\n",
        "total_triton_time = 0.0\n",
        "num_runs = 100\n",
        "\n",
        "for i in range(num_runs):\n",
        "    torch.manual_seed(i)\n",
        "\n",
        "    x = torch.randn(batch_size, seq_len, num_heads, head_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    cos = torch.randn(rope_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    sin = torch.randn_like(cos)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start_ref = torch.cuda.Event(enable_timing=True)\n",
        "    end_ref = torch.cuda.Event(enable_timing=True)\n",
        "    start_ref.record()\n",
        "    ref = apply_rotary_pos_emb(x, (cos, sin))\n",
        "    end_ref.record()\n",
        "    torch.cuda.synchronize()\n",
        "    ref_time = start_ref.elapsed_time(end_ref)\n",
        "    total_ref_time += ref_time\n",
        "\n",
        "    start_tri = torch.cuda.Event(enable_timing=True)\n",
        "    end_tri = torch.cuda.Event(enable_timing=True)\n",
        "    start_tri.record()\n",
        "    tri = apply_rotary_pos_emb_triton(x, (cos, sin))\n",
        "    end_tri.record()\n",
        "    torch.cuda.synchronize()\n",
        "    triton_time = start_tri.elapsed_time(end_tri)\n",
        "    total_triton_time += triton_time\n",
        "\n",
        "    print(ref_time)\n",
        "    print(triton_time)\n",
        "    print()\n",
        "\n",
        "    diff = (ref - tri).abs().max()\n",
        "\n",
        "average_ref_time = total_ref_time / num_runs\n",
        "average_triton_time = total_triton_time / num_runs\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Average ref time over {num_runs} runs: {average_ref_time:.4f} ms\")\n",
        "print(f\"Average triton time over {num_runs} runs: {average_triton_time:.4f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQkHCTSF_peb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
