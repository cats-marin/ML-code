{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install triton==3.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "ueQ-9xWeF4Le",
        "outputId": "a7b3db71-45a3-49be-d9de-3b2859193dba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.3.1\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1) (75.2.0)\n",
            "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed triton-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "1c263dfec2e041cc8e8cd79d57a9dfcf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8DL4w3AM8CPo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.autotune(\n",
        "    configs=[\n",
        "        triton.Config({'BLOCK_D': 16}, num_warps=1),\n",
        "        triton.Config({'BLOCK_D': 16}, num_warps=2),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 32}, num_warps=2),\n",
        "        triton.Config({'BLOCK_D': 32}, num_warps=4),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 64}, num_warps=1),\n",
        "        triton.Config({'BLOCK_D': 64}, num_warps=2),\n",
        "        triton.Config({'BLOCK_D': 64}, num_warps=4),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 128}, num_warps=2),\n",
        "        triton.Config({'BLOCK_D': 128}, num_warps=4),\n",
        "        triton.Config({'BLOCK_D': 128}, num_warps=8),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 256}, num_warps=4),\n",
        "        triton.Config({'BLOCK_D': 256}, num_warps=8),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 512}, num_warps=4),\n",
        "        triton.Config({'BLOCK_D': 512}, num_warps=8),\n",
        "\n",
        "        triton.Config({'BLOCK_D': 1024}, num_warps=8),\n",
        "        triton.Config({'BLOCK_D': 1024}, num_warps=16),\n",
        "    ],\n",
        "    key=['HEAD_DIM', 'ROPE_DIM', 'NUM_ELEMENTS'],\n",
        ")\n",
        "\n",
        "@triton.jit\n",
        "def _rope_fused_kernel(\n",
        "    x_ptr, cos_ptr, sin_ptr, out_ptr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    ROPE_DIM: tl.constexpr,\n",
        "    NUM_ELEMENTS: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    offs_d = tl.arange(0, BLOCK_D)\n",
        "    offs = tl.program_id(axis=0) * BLOCK_D + offs_d\n",
        "\n",
        "    is_in_bounds = offs < NUM_ELEMENTS\n",
        "    x = tl.load(x_ptr + offs, mask=is_in_bounds, other=0.0)\n",
        "\n",
        "    ROPE_OFFSET: tl.constexpr = HEAD_DIM - ROPE_DIM\n",
        "    offs_rope = (offs_d % HEAD_DIM) - ROPE_OFFSET\n",
        "\n",
        "    HALF_ROPE_DIM: tl.constexpr = ROPE_DIM // 2\n",
        "    is_first_half = offs_rope < HALF_ROPE_DIM\n",
        "    rope_partner = tl.gather(x, offs_d + tl.where(is_first_half, HALF_ROPE_DIM, -HALF_ROPE_DIM), axis=0)\n",
        "\n",
        "    use_rope = offs_rope >= 0\n",
        "    cos = tl.load(cos_ptr + offs_rope, mask=use_rope, other=0.0)\n",
        "    sin = tl.load(sin_ptr + offs_rope, mask=use_rope, other=0.0)\n",
        "\n",
        "    out = tl.where(use_rope, x * cos + tl.where(is_first_half, -1.0,  1.0) * rope_partner * sin, x)\n",
        "    tl.store(out_ptr + offs, out, mask=is_in_bounds)"
      ],
      "metadata": {
        "id": "vNzcGCTr8I5o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @triton.autotune(\n",
        "#     configs=[\n",
        "#         triton.Config({'XBLOCK': 32}, num_warps=2),\n",
        "#         triton.Config({'XBLOCK': 64}, num_warps=1),\n",
        "#         triton.Config({'XBLOCK': 128}, num_warps=4),\n",
        "#         triton.Config({'XBLOCK': 256}, num_warps=8),\n",
        "#     ],\n",
        "#     key=['xnumel'],\n",
        "# )\n",
        "# @triton.jit\n",
        "# def triton_poi_fused_cat_0(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
        "#     xnumel = 16777216\n",
        "#     xoffset = tl.program_id(0) * XBLOCK\n",
        "#     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
        "#     xmask = tl.full([XBLOCK], True, tl.int1)\n",
        "#     x0 = (xindex % 64)\n",
        "#     x1 = xindex // 64\n",
        "#     x2 = xindex\n",
        "#     tmp0 = x0\n",
        "#     tmp1 = tl.full([1], 0, tl.int64)\n",
        "#     tmp2 = tmp0 >= tmp1\n",
        "#     tmp3 = tl.full([1], 16, tl.int64)\n",
        "#     tmp4 = tmp0 < tmp3\n",
        "#     tmp5 = tl.load(in_ptr0 + (64*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp6 = tmp0 >= tmp3\n",
        "#     tmp7 = tl.full([1], 64, tl.int64)\n",
        "#     tmp8 = tmp0 < tmp7\n",
        "#     tmp9 = tl.load(in_ptr0 + (16 + 64*x1 + ((-16) + x0)), tmp6, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp10 = tl.load(in_ptr1 + ((-16) + x0), tmp6, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp11 = tmp9 * tmp10\n",
        "#     tmp12 = (-16) + x0\n",
        "#     tmp13 = tl.full([1], 0, tl.int64)\n",
        "#     tmp14 = tmp12 >= tmp13\n",
        "#     tmp15 = tl.full([1], 24, tl.int64)\n",
        "#     tmp16 = tmp12 < tmp15\n",
        "#     tmp17 = tmp16 & tmp6\n",
        "#     tmp18 = tl.load(in_ptr0 + (40 + 64*x1 + ((-16) + x0)), tmp17, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp19 = -tmp18\n",
        "#     tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)\n",
        "#     tmp21 = tl.where(tmp17, tmp19, tmp20)\n",
        "#     tmp22 = tmp12 >= tmp15\n",
        "#     tmp23 = tl.full([1], 48, tl.int64)\n",
        "#     tmp24 = tmp12 < tmp23\n",
        "#     tmp25 = tmp22 & tmp6\n",
        "#     tmp26 = tl.load(in_ptr0 + (16 + 64*x1 + ((-24) + ((-16) + x0))), tmp25, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp27 = tl.where(tmp16, tmp21, tmp26)\n",
        "#     tmp28 = tl.load(in_ptr2 + ((-16) + x0), tmp6, eviction_policy='evict_last', other=0.0)\n",
        "#     tmp29 = tmp27 * tmp28\n",
        "#     tmp30 = tmp11 + tmp29\n",
        "#     tmp31 = tl.full(tmp30.shape, 0.0, tmp30.dtype)\n",
        "#     tmp32 = tl.where(tmp6, tmp30, tmp31)\n",
        "#     tmp33 = tl.where(tmp4, tmp5, tmp32)\n",
        "#     tl.store(out_ptr0 + (x2), tmp33, None)\n"
      ],
      "metadata": {
        "id": "OzjWr-0jCpzH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_pos_emb_triton(\n",
        "    x: torch.Tensor,\n",
        "    cos_sin: Tuple[torch.Tensor, torch.Tensor],\n",
        ") -> torch.Tensor:\n",
        "    if x.device.type != \"cuda\":\n",
        "        raise RuntimeError(\"Triton kernel requires CUDA tensor\")\n",
        "    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n",
        "        raise TypeError(\"x must be fp16, bf16, or fp32\")\n",
        "\n",
        "    cos, sin = cos_sin\n",
        "\n",
        "    head_dim = x.size(-1)\n",
        "    rope_dim = cos.size(-1)\n",
        "\n",
        "    if rope_dim % 2:\n",
        "        raise ValueError(\"rope_dim must be even\")\n",
        "    if rope_dim > head_dim:\n",
        "        raise ValueError(\"rope_dim should be less than or equal to head_dim\")\n",
        "\n",
        "    out = torch.empty_like(x)\n",
        "    x_numel = x.numel()\n",
        "\n",
        "    grid = lambda META: (triton.cdiv(x_numel, META['BLOCK_D']),)\n",
        "\n",
        "    _rope_fused_kernel[grid](\n",
        "        x, cos, sin, out,\n",
        "        head_dim,\n",
        "        rope_dim,\n",
        "        x_numel,\n",
        "    )\n",
        "\n",
        "    return out.view_as(x)"
      ],
      "metadata": {
        "id": "LfiC9vbO8Uqs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_pos_emb(\n",
        "    x: torch.Tensor, cos_sin: tuple[torch.Tensor, torch.Tensor]\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    cos, sin = cos_sin\n",
        "\n",
        "    head_dim = x.size(-1)\n",
        "    rope_dim = cos.size(-1)\n",
        "\n",
        "    if head_dim == rope_dim:\n",
        "        x = (x * cos) + (_rotate_half(x) * sin)\n",
        "    elif rope_dim < head_dim:\n",
        "        x_nope, x_rope = x.split((head_dim - rope_dim, rope_dim), dim=-1)\n",
        "        x_rope = (x_rope * cos) + (_rotate_half(x_rope) * sin)\n",
        "        x = torch.cat([x_nope, x_rope], dim=-1)\n",
        "    else:\n",
        "        raise ValueError(\"rope_dim should be less than head_dim\")\n",
        "\n",
        "    return x\n",
        "\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)"
      ],
      "metadata": {
        "id": "NtKYRBeY8WCQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    batch_size = 32\n",
        "    seq_len = 1024\n",
        "    num_heads = 8\n",
        "    head_dim = 64\n",
        "    rope_dim = 48\n",
        "\n",
        "    x = torch.randn(batch_size, seq_len, num_heads, head_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    cos = torch.randn(rope_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    sin = torch.randn_like(cos)\n",
        "\n",
        "    ref = apply_rotary_pos_emb(x, (cos, sin))\n",
        "    tri = apply_rotary_pos_emb_triton(x, (cos, sin))\n",
        "\n",
        "    diff = (ref - tri).abs().max()\n",
        "    print(\"max|diff| =\", diff.item())\n",
        "\n",
        "    ref = (apply_rotary_pos_emb)(x, (cos, sin))\n",
        "    tri = (apply_rotary_pos_emb_triton)(x, (cos, sin))\n",
        "\n",
        "    diff = (ref - tri).abs().max()\n",
        "    print(\"max|diff| =\", diff.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QLrf8Jo8XdN",
        "outputId": "ee6b3607-0bf1-4e99-ad4a-184d28e2f612"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max|diff| = 9.5367431640625e-07\n",
            "max|diff| = 9.5367431640625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "apply_rotary_pos_emb = (apply_rotary_pos_emb)\n",
        "apply_rotary_pos_emb_triton = (apply_rotary_pos_emb_triton)\n",
        "\n",
        "batch_size = 32\n",
        "seq_len = 1024\n",
        "num_heads = 8\n",
        "head_dim = 64\n",
        "rope_dim = 64\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, num_heads, head_dim, device=\"cuda\", dtype=torch.float32)\n",
        "cos = torch.randn(rope_dim, device=\"cuda\", dtype=torch.float32)\n",
        "sin = torch.randn_like(cos)\n",
        "\n",
        "for _ in range(0):\n",
        "    _ = apply_rotary_pos_emb(x, (cos, sin))\n",
        "    _ = apply_rotary_pos_emb_triton(x, (cos, sin))\n",
        "\n",
        "total_ref_time = 0.0\n",
        "total_triton_time = 0.0\n",
        "num_runs = 100\n",
        "\n",
        "for i in range(num_runs):\n",
        "    torch.manual_seed(i)\n",
        "\n",
        "    x = torch.randn(batch_size, seq_len, num_heads, head_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    cos = torch.randn(rope_dim, device=\"cuda\", dtype=torch.float32)\n",
        "    sin = torch.randn_like(cos)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start_ref = torch.cuda.Event(enable_timing=True)\n",
        "    end_ref = torch.cuda.Event(enable_timing=True)\n",
        "    start_ref.record()\n",
        "    ref = apply_rotary_pos_emb(x, (cos, sin))\n",
        "    end_ref.record()\n",
        "    torch.cuda.synchronize()\n",
        "    ref_time = start_ref.elapsed_time(end_ref)\n",
        "    total_ref_time += ref_time\n",
        "\n",
        "    start_tri = torch.cuda.Event(enable_timing=True)\n",
        "    end_tri = torch.cuda.Event(enable_timing=True)\n",
        "    start_tri.record()\n",
        "    tri = apply_rotary_pos_emb_triton(x, (cos, sin))\n",
        "    end_tri.record()\n",
        "    torch.cuda.synchronize()\n",
        "    triton_time = start_tri.elapsed_time(end_tri)\n",
        "    total_triton_time += triton_time\n",
        "\n",
        "    print(triton_time)\n",
        "    print()\n",
        "\n",
        "    diff = (ref - tri).abs().max()\n",
        "\n",
        "average_ref_time = total_ref_time / num_runs\n",
        "average_triton_time = total_triton_time / num_runs\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Average ref time over {num_runs} runs: {average_ref_time:.4f} ms\")\n",
        "print(f\"Average triton time over {num_runs} runs: {average_triton_time:.4f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3np4GAH-Vc8",
        "outputId": "11ab3945-9bf9-43de-d0a5-b30206ba6309"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2008.0614013671875\n",
            "\n",
            "0.6913279891014099\n",
            "\n",
            "0.6683520078659058\n",
            "\n",
            "0.654591977596283\n",
            "\n",
            "0.6594560146331787\n",
            "\n",
            "0.653439998626709\n",
            "\n",
            "0.6574079990386963\n",
            "\n",
            "0.6570879817008972\n",
            "\n",
            "0.6480960249900818\n",
            "\n",
            "0.6518719792366028\n",
            "\n",
            "0.6631039977073669\n",
            "\n",
            "0.6541119813919067\n",
            "\n",
            "0.6512960195541382\n",
            "\n",
            "0.6559360027313232\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.647167980670929\n",
            "\n",
            "0.6500160098075867\n",
            "\n",
            "0.6635519862174988\n",
            "\n",
            "0.6922240257263184\n",
            "\n",
            "0.6522240042686462\n",
            "\n",
            "0.662559986114502\n",
            "\n",
            "0.6615039706230164\n",
            "\n",
            "0.6594560146331787\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.6600639820098877\n",
            "\n",
            "0.6556159853935242\n",
            "\n",
            "0.6601920127868652\n",
            "\n",
            "0.657696008682251\n",
            "\n",
            "0.6525760293006897\n",
            "\n",
            "0.6711999773979187\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.6528639793395996\n",
            "\n",
            "0.6573759913444519\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.8942080140113831\n",
            "\n",
            "0.6574079990386963\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.6531199812889099\n",
            "\n",
            "0.6461759805679321\n",
            "\n",
            "0.646336019039154\n",
            "\n",
            "0.650111973285675\n",
            "\n",
            "0.6647359728813171\n",
            "\n",
            "0.6470080018043518\n",
            "\n",
            "0.7101439833641052\n",
            "\n",
            "0.7275840044021606\n",
            "\n",
            "0.7017920017242432\n",
            "\n",
            "0.7167999744415283\n",
            "\n",
            "0.7043520212173462\n",
            "\n",
            "0.6613439917564392\n",
            "\n",
            "0.6768320202827454\n",
            "\n",
            "0.663424015045166\n",
            "\n",
            "0.6470720171928406\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.6848000288009644\n",
            "\n",
            "0.6547840237617493\n",
            "\n",
            "0.6432639956474304\n",
            "\n",
            "0.6451200246810913\n",
            "\n",
            "0.6479679942131042\n",
            "\n",
            "0.6452479958534241\n",
            "\n",
            "0.6594560146331787\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.6671040058135986\n",
            "\n",
            "0.6552960276603699\n",
            "\n",
            "0.669376015663147\n",
            "\n",
            "0.6557760238647461\n",
            "\n",
            "0.681984007358551\n",
            "\n",
            "0.6470400094985962\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.6533120274543762\n",
            "\n",
            "0.6612480282783508\n",
            "\n",
            "0.6618880033493042\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.6502079963684082\n",
            "\n",
            "0.6446080207824707\n",
            "\n",
            "0.6533120274543762\n",
            "\n",
            "0.6470720171928406\n",
            "\n",
            "0.6446080207824707\n",
            "\n",
            "0.6904320120811462\n",
            "\n",
            "0.6592000126838684\n",
            "\n",
            "0.649183988571167\n",
            "\n",
            "0.6475840210914612\n",
            "\n",
            "0.6677119731903076\n",
            "\n",
            "0.6472319960594177\n",
            "\n",
            "0.6475840210914612\n",
            "\n",
            "0.6451200246810913\n",
            "\n",
            "0.6553599834442139\n",
            "\n",
            "0.6468480229377747\n",
            "\n",
            "0.655135989189148\n",
            "\n",
            "0.6582080125808716\n",
            "\n",
            "0.662015974521637\n",
            "\n",
            "0.6452479958534241\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.647167980670929\n",
            "\n",
            "0.6492159962654114\n",
            "\n",
            "0.6531199812889099\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "0.652895987033844\n",
            "\n",
            "0.6512640118598938\n",
            "\n",
            "------------------------------\n",
            "Average ref time over 100 runs: 2.9780 ms\n",
            "Average triton time over 100 runs: 20.7355 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQkHCTSF_peb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}