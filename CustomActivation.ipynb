{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dca9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from torch.autograd import Function\n",
    "import torch.utils.benchmark as benchmark\n",
    "import torch._dynamo\n",
    "import torch._inductor.metrics as metrics\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, num_features, num_control_points=1000, eps=1e-6):\n",
    "        super(CustomActivation, self).__init__()\n",
    "        self.register_buffer(\"mins\", None)\n",
    "        self.register_buffer(\"maxs\", None)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.num_control_points = num_control_points\n",
    "\n",
    "        self.r_weight = nn.Parameter(torch.zeros(num_features, num_control_points))  # (num_features, num_control_points)\n",
    "        self.l_weight = nn.Parameter(torch.zeros(num_features, num_control_points))  # (num_features, num_control_points)\n",
    "\n",
    "        self.register_buffer(\"local_bias\", torch.arange(num_control_points))  # (num_control_points,)\n",
    "        self.register_buffer(\"feature_offset\", torch.arange(num_features).view(1, -1) * self.num_control_points)  # (1, num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_features)\n",
    "\n",
    "        if self.training or self.mins is None or self.maxs is None:\n",
    "            self.mins = x.amin(dim=0, keepdim=True)  # (1, num_features)\n",
    "            self.maxs = x.amax(dim=0, keepdim=True)  # (1, num_features)\n",
    "\n",
    "        x = (x - self.mins) / (self.maxs - self.mins + self.eps) * (self.num_control_points - 1)  # (batch_size, num_features)\n",
    "\n",
    "        # TODO: may change to feature-major order (num_features, batch_size) since that may help with memory access patterns (improved locality)\n",
    "        \n",
    "        lower_indices_float = x.floor().clamp(0, self.num_control_points - 2)  # (batch_size, num_features)\n",
    "        lower_indices = lower_indices_float.long() + self.feature_offset  # (batch_size, num_features)\n",
    "\n",
    "        indices = torch.stack((lower_indices, lower_indices + 1), dim=-1)  # (batch_size, num_features, 2)\n",
    "        vals = F.embedding(indices, self.get_interp_tensor())  # (batch_size, num_features, 2, 1)\n",
    "\n",
    "        lower_val, upper_val = vals.squeeze(-1).unbind(-1)  # each: (batch_size, num_features)\n",
    "        return torch.lerp(lower_val, upper_val, x - lower_indices_float)  # (batch_size, num_features)\n",
    "    \n",
    "    def get_interp_tensor(self):\n",
    "        cs_r_weight = torch.cumsum(self.r_weight, dim=1)  # (num_features, num_control_points)\n",
    "        cs_l_weight = torch.cumsum(self.l_weight, dim=1)  # (num_features, num_control_points)\n",
    "\n",
    "        cs_r_weight_bias_prod = torch.cumsum(self.r_weight * self.local_bias, dim=1)  # type: ignore (num_features, num_control_points)\n",
    "        cs_l_weight_bias_prod = torch.cumsum(self.l_weight * self.local_bias, dim=1)  # type: ignore (num_features, num_control_points)\n",
    "\n",
    "        r_interp = (self.local_bias * cs_r_weight - cs_r_weight_bias_prod)  # type: ignore (num_features, num_control_points)\n",
    "        l_interp = (cs_l_weight_bias_prod[..., -1:] - cs_l_weight_bias_prod) - self.local_bias * (cs_l_weight[..., -1:] - cs_l_weight)  # type: ignore (num_features, num_control_points)\n",
    "        return (r_interp + l_interp).view(-1, 1)  # (num_features * num_control_points, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511229a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
