{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d704e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from torch.autograd import Function\n",
    "import torch.utils.benchmark as benchmark\n",
    "import torch._dynamo\n",
    "import torch._inductor.metrics as metrics\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde6aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(model, x, y, optimizer, loss_fn, iterations, device):\n",
    "    \"\"\"\n",
    "    Benchmarks the forward and backward pass of a given PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to benchmark.\n",
    "        x (torch.Tensor): The input data.\n",
    "        y (torch.Tensor): The target data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the backward pass.\n",
    "        loss_fn: The loss function.\n",
    "        iterations (int): The number of iterations to run the benchmark.\n",
    "        device (str): The device to run the benchmark on ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    forward_times = []\n",
    "    backward_times = []\n",
    "\n",
    "    # Warm-up iterations to handle initial CUDA overhead, etc.\n",
    "    for _ in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Ensure all CUDA operations are synchronized before starting the timer\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # --- Benchmarking Loop ---\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Benchmark Forward Pass\n",
    "        start_time = time.perf_counter()\n",
    "        output = model(x)\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        forward_times.append(end_time - start_time)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # Benchmark Backward Pass\n",
    "        start_time = time.perf_counter()\n",
    "        loss.backward()\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        backward_times.append(end_time - start_time)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # --- Report Results ---\n",
    "    avg_forward = sum(forward_times) / iterations * 1000  # Convert to ms\n",
    "    avg_backward = sum(backward_times) / iterations * 1000 # Convert to ms\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Benchmarking Results on '{device.upper()}'\")\n",
    "    print(f\"Iterations: {iterations}\")\n",
    "    print(f\"Average Forward Pass Time:  {avg_forward:.4f} ms\")\n",
    "    print(f\"Average Backward Pass Time: {avg_backward:.4f} ms\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7a46c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "NUM_FEATURES = 10\n",
    "NUM_CONTROL_POINTS = 1000\n",
    "ITERATIONS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create random input and target tensors\n",
    "x_input = torch.randn(BATCH_SIZE, NUM_FEATURES).to(DEVICE)\n",
    "y_target = torch.randn(BATCH_SIZE, NUM_FEATURES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29373f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, num_features, num_control_points=1000, eps=1e-6):\n",
    "        super(CustomActivation, self).__init__()\n",
    "        self.register_buffer(\"mins\", None)\n",
    "        self.register_buffer(\"maxs\", None)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.num_control_points = num_control_points\n",
    "\n",
    "        self.r_weight = nn.Parameter(torch.zeros(num_features, num_control_points))  # (num_features, num_control_points)\n",
    "        self.l_weight = nn.Parameter(torch.zeros(num_features, num_control_points))  # (num_features, num_control_points)\n",
    "\n",
    "        self.register_buffer(\"local_bias\", torch.arange(num_control_points))  # (num_control_points,)\n",
    "        self.register_buffer(\"feature_offset\", torch.arange(num_features).view(1, -1) * self.num_control_points)  # (1, num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_features)\n",
    "\n",
    "        if self.training or self.mins is None or self.maxs is None:\n",
    "            self.mins = x.amin(dim=0, keepdim=True)  # (1, num_features)\n",
    "            self.maxs = x.amax(dim=0, keepdim=True)  # (1, num_features)\n",
    "\n",
    "        x = (x - self.mins) / (self.maxs - self.mins + self.eps) * (self.num_control_points - 1)  # (batch_size, num_features)\n",
    "\n",
    "        # TODO: may change to feature-major order (num_features, batch_size) since that may help with memory access patterns (improved locality)\n",
    "        \n",
    "        lower_indices_float = x.floor().clamp(0, self.num_control_points - 2)  # (batch_size, num_features)\n",
    "        lower_indices = lower_indices_float.long() + self.feature_offset  # (batch_size, num_features)\n",
    "\n",
    "        indices = torch.stack((lower_indices, lower_indices + 1), dim=-1)  # (batch_size, num_features, 2)\n",
    "        vals = F.embedding(indices, self.get_interp_tensor())  # (batch_size, num_features, 2, 1)\n",
    "\n",
    "        lower_val, upper_val = vals.squeeze(-1).unbind(-1)  # each: (batch_size, num_features)\n",
    "        return torch.lerp(lower_val, upper_val, x - lower_indices_float)  # (batch_size, num_features)\n",
    "    \n",
    "    def get_interp_tensor(self):\n",
    "        cs_r_weight = torch.cumsum(self.r_weight, dim=1)  # (num_features, num_control_points)\n",
    "        cs_l_weight = torch.cumsum(self.l_weight, dim=1)  # (num_features, num_control_points)\n",
    "\n",
    "        cs_r_weight_bias_prod = torch.cumsum(self.r_weight * self.local_bias, dim=1)  # type: ignore (num_features, num_control_points)\n",
    "        cs_l_weight_bias_prod = torch.cumsum(self.l_weight * self.local_bias, dim=1)  # type: ignore (num_features, num_control_points)\n",
    "\n",
    "        r_interp = (self.local_bias * cs_r_weight - cs_r_weight_bias_prod)  # type: ignore (num_features, num_control_points)\n",
    "        l_interp = (cs_l_weight_bias_prod[..., -1:] - cs_l_weight_bias_prod) - self.local_bias * (cs_l_weight[..., -1:] - cs_l_weight)  # type: ignore (num_features, num_control_points)\n",
    "        return (r_interp + l_interp).view(-1, 1)  # (num_features * num_control_points, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c572aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Benchmarking Results on 'CPU'\n",
      "Iterations: 100\n",
      "Average Forward Pass Time:  0.3904 ms\n",
      "Average Backward Pass Time: 0.5963 ms\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "# Instantiate the model and move it to the selected device\n",
    "model = CustomActivation(\n",
    "    num_features=NUM_FEATURES,\n",
    "    num_control_points=NUM_CONTROL_POINTS\n",
    ").to(DEVICE)\n",
    "model.train() # Set model to training mode\n",
    "\n",
    "# Define a simple loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Run Benchmark ---\n",
    "benchmark(model, x_input, y_target, optimizer, loss_function, ITERATIONS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, num_features, num_control_points):\n",
    "        super(CustomActivation, self).__init__()\n",
    "        # self.register_buffer('local_bias', torch.linspace(1, num_control_points * 2 - 1, num_control_points).expand(num_features, -1))\n",
    "        self.register_buffer('local_bias', torch.linspace(-5, 5, num_control_points).expand(num_features, -1))\n",
    "\n",
    "        self.pos_weight = nn.Parameter(torch.zeros(num_features, num_control_points))\n",
    "        self.neg_weight = nn.Parameter(torch.zeros(num_features, num_control_points))\n",
    "\n",
    "        self.global_weight = nn.Parameter(torch.zeros(1, num_features))\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1, num_features))\n",
    "\n",
    "        self.x_nonlinear_mean = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shifted = x.unsqueeze(-1) + self.local_bias\n",
    "        x_nonlinear = (F.relu(x_shifted) * self.pos_weight).sum(dim=-1) + (F.relu(-x_shifted) * self.neg_weight).sum(dim=-1)\n",
    "        return x_nonlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97416dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Benchmarking Results on 'CPU'\n",
      "Iterations: 100\n",
      "Average Forward Pass Time:  12.1262 ms\n",
      "Average Backward Pass Time: 5.7455 ms\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "# Instantiate the model and move it to the selected device\n",
    "model = CustomActivation(\n",
    "    num_features=NUM_FEATURES,\n",
    "    num_control_points=NUM_CONTROL_POINTS\n",
    ").to(DEVICE)\n",
    "model.train() # Set model to training mode\n",
    "\n",
    "# Define a simple loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Run Benchmark ---\n",
    "benchmark(model, x_input, y_target, optimizer, loss_function, ITERATIONS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee679308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
